from fastapi import FastAPI, Depends, HTTPException, Query
from pydantic import BaseModel
from typing import Optional
from fastapi.middleware.cors import CORSMiddleware
from openai import OpenAI
from dotenv import load_dotenv
import os
from pathlib import Path
from sqlalchemy import Column, Integer, String, Text, create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session

# 1. Configura√ß√µes Iniciais e Vari√°veis de Ambiente
env_path = Path(__file__).parent / ".env"
load_dotenv(dotenv_path=env_path)

app = FastAPI()

# Configura√ß√£o do CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 2. Configura√ß√£o de IA (Groq / OpenAI / Ollama)
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
API_KEY = os.getenv("API_KEY")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434/v1")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama3.2")

if GROQ_API_KEY:
    client = OpenAI(api_key=GROQ_API_KEY, base_url="https://api.groq.com/openai/v1")
    AI_MODEL = os.getenv("GROQ_MODEL", "llama-3.1-8b-instant")
    AI_PROVIDER = "Groq"
elif API_KEY:
    client = OpenAI(api_key=API_KEY)
    AI_MODEL = "gpt-3.5-turbo"
    AI_PROVIDER = "OpenAI"
else:
    client = OpenAI(api_url=OLLAMA_BASE_URL, api_key="ollama")
    AI_MODEL = OLLAMA_MODEL
    AI_PROVIDER = "Ollama"

print(f"[IA] Provedor: {AI_PROVIDER} | Modelo Padr√£o: {AI_MODEL}")

# ----------------- 3. SQLAlchemy / SQLite -----------------
# O banco fica no /tmp para funcionar na Vercel
SQLALCHEMY_DATABASE_URL = "sqlite:////tmp/db.sqlite3"
engine = create_engine(
    SQLALCHEMY_DATABASE_URL, connect_args={"check_same_thread": False}
)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

# Modelo da tabela com o novo campo user_id
class TitleModel(Base):
    __tablename__ = "titles"
    id = Column(Integer, primary_key=True, index=True)
    titulo = Column(String, nullable=False)
    descricao = Column(Text, nullable=False)
    user_id = Column(String, index=True, nullable=False) # Identificador do navegador

Base.metadata.create_all(bind=engine)

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# ----------------- 4. Schemas Pydantic -----------------
class Produto(BaseModel):
    # Todos agora s√£o opcionais, pois o usu√°rio pode mandar s√≥ a foto
    categoria: Optional[str] = ""
    beneficios: Optional[str] = ""
    material: Optional[str] = ""
    imagem: Optional[str] = None # Campo para receber a imagem em base64

class TitleCreate(BaseModel):
    titulo: str
    descricao: str
    user_id: str # Campo obrigat√≥rio ao salvar

class TitleResponse(BaseModel):
    id: int
    titulo: str
    descricao: str
    user_id: str

    class Config:
        from_attributes = True # Compat√≠vel com SQLAlchemy

# ----------------- 5. Endpoints de CRUD -----------------

@app.get("/titles", response_model=list[TitleResponse])
def read_titles(user_id: str = Query(...), db: Session = Depends(get_db)):
    """Busca t√≠tulos filtrando pelo ID √∫nico do usu√°rio/navegador"""
    return db.query(TitleModel).filter(TitleModel.user_id == user_id).order_by(TitleModel.id.desc()).all()

@app.post("/titles", response_model=TitleResponse)
def create_title(item: TitleCreate, db: Session = Depends(get_db)):
    """Salva um t√≠tulo vinculado a um user_id"""
    db_obj = TitleModel(
        titulo=item.titulo, 
        descricao=item.descricao, 
        user_id=item.user_id
    )
    db.add(db_obj)
    db.commit()
    db.refresh(db_obj)
    return db_obj

@app.delete("/titles/{title_id}", status_code=204)
def delete_title(title_id: int, db: Session = Depends(get_db)):
    db_obj = db.query(TitleModel).filter(TitleModel.id == title_id).first()
    if not db_obj:
        raise HTTPException(status_code=404, detail="T√≠tulo n√£o encontrado")
    db.delete(db_obj)
    db.commit()

# ----------------- 6. Rota de Gera√ß√£o de IA -----------------

@app.post("/gerar")
def gerar_titulo_descricao(produto: Produto):
    try:
        # Define o modelo atual que vai ser usado na requisi√ß√£o
        current_model = AI_MODEL

        prompt_text = (
            f"Crie um t√≠tulo e uma descri√ß√£o CURTOS e ALTAMENTE VENDEDORES para um produto.\n"
            "Se houver uma imagem fornecida, analise a imagem e descreva o produto que voc√™ v√™ nela, combinando com as informa√ß√µes abaixo (se existirem):\n"
            f"- Categoria: {produto.categoria or 'N√£o informada'}\n"
            f"- Benef√≠cios/Detalhes extras: {produto.beneficios or 'N√£o informados'}\n"
            f"- Material: {produto.material or 'N√£o informado'}\n\n"
            "Regras: retorne APENAS texto puro, sem Markdown. "
            "T√≠tulo: atrativo, m√°ximo 60 caracteres.\n"
            "Descri√ß√£o: m√°ximo 3 frases objetivas focadas em convers√£o.\n"
            "Formato exato de resposta:\nT√≠tulo: (texto)\nDescri√ß√£o: (texto)"
        )

        # Se o usu√°rio enviou imagem, mudamos a estrutura da mensagem
        if produto.imagem:
            user_content = [
                {"type": "text", "text": prompt_text},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": produto.imagem # J√° vem como data:image/jpeg;base64,... do React
                    }
                }
            ]
            
            # --- CORRE√á√ÉO DO MODELO DE VIS√ÉO ---
            if AI_PROVIDER == "Groq":
                # Utiliza o modelo de vis√£o mais recente recomendado pela Groq (Llama 4 Scout)
                current_model = os.getenv("GROQ_VISION_MODEL", "meta-llama/llama-4-scout-17b-16e-instruct")
            elif AI_PROVIDER == "OpenAI":
                current_model = "gpt-4o-mini"
            
            print(f"üì∏ Imagem recebida! Usando modelo de vis√£o: {current_model}")
            
        else:
            # Se n√£o tem imagem, envia s√≥ a string normal
            user_content = prompt_text
            print(f"üìù Apenas texto recebido. Usando modelo: {current_model}")

        # Faz a chamada para a IA
        response = client.chat.completions.create(
            model=current_model,
            messages=[
                {"role": "system", "content": "Voc√™ √© um Copywriter Especialista em E-commerce e An√°lise de Imagens."},
                {"role": "user", "content": user_content}
            ],
            temperature=0.7,
            max_tokens=400 # Boa pr√°tica para vis√£o
        )

        resultado = response.choices[0].message.content if response.choices else "Erro na IA"
        return {"resultado": resultado}

    except Exception as e:
        print(f"Erro no backend: {str(e)}") # √ötil para debug no terminal
        return {"resultado": f"Erro ao gerar conte√∫do: {str(e)}"}